{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Przygotowanie danych historycznych.\n",
        "Główne funkcjonalności: \\\n",
        "1) czytywanie danych giełdowych z wielu plików CSV. \\\n",
        "2) Dodawanie informacji o tickerze (symbolu akcji). \\\n",
        "3) Filtrowanie danych na podstawie liczby rekordów i zakresu dat. \\\n",
        "4) Obliczanie zwrotu z inwestycji w oknach czasowych."
      ],
      "metadata": {
        "id": "hkAGWfQTjvZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "from datetime import datetime\n",
        "import os\n",
        "from pyspark.sql.functions import to_date\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BstA3idJW9Kq",
        "outputId": "04a8d724-6444-4b7a-b6a6-49ecfdfe6ce8"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Stocks Data Analysis\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "2yZo3dLLemQ6"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")"
      ],
      "metadata": {
        "id": "0owTql5dxzxK"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Schemat dla plików CSV\n",
        "schema = StructType([\n",
        "    StructField(\"Date\", StringType(), True),\n",
        "    StructField(\"Open\", DoubleType(), True),\n",
        "    StructField(\"Close\", DoubleType(), True)\n",
        "])"
      ],
      "metadata": {
        "id": "PY96X5TheijD"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_file(file_path, n_steps):\n",
        "    df = spark.read.csv(file_path, schema=schema, header=False)\n",
        "\n",
        "    df = df.withColumn(\"Symbol\", F.input_file_name()) \\\n",
        "        .withColumn(\"Symbol\", F.element_at(F.split(F.col(\"Symbol\"), \"/\"), -1)) \\\n",
        "        .withColumn(\"Symbol\", F.element_at(F.split(F.col(\"Symbol\"), \"\\\\.\"), 1))\n",
        "\n",
        "    # Zliczana jest liczba wierszy dla każdego Symbol.\n",
        "    # Pliki z mniej niż 5 latami danych (minimum 260 dni giełdowych w roku + margines 10) są odrzucane.\n",
        "    df = df.withColumn('count', F.count(\"Symbol\").over(Window.partitionBy('Symbol'))) \\\n",
        "        .filter(F.col('count') > 260 * 5 + 10)\n",
        "\n",
        "    df = df.withColumn(\"Date\", F.to_date(F.to_timestamp(F.col('Date'), 'yy-MM-dd')))\n",
        "    df = df.filter(F.col(\"Date\") >= datetime(2013, 1, 2)) \\\n",
        "           .filter(F.col('Date') <= datetime(2023, 12, 29))\n",
        "\n",
        "    # Funkcja do obliczania zwrotu (rolling window)\n",
        "    window = Window.partitionBy(\"Symbol\").orderBy(\"Date\").rowsBetween(1 - n_steps, 0)\n",
        "    df = df.withColumn(\"return\", \\\n",
        "                       (F.col(\"Close\") - F.first(\"Close\").over(window)) / F.first(\"Close\").over(window))\n",
        "\n",
        "    return df.orderBy(\"Symbol\", \"Date\")"
      ],
      "metadata": {
        "id": "M7qGkUWvee5h"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obliczany jest zwrot z inwestycji (return) jako procentowa zmiana wartości w oknie czasowym o długości n_steps dni.\n",
        "# Używane są okna czasowe (Window), które grupują dane według Symbol i sortują według daty.\n",
        "\n",
        "n_steps = 10\n",
        "input_dir = '/content/drive/My Drive/BigDATA'"
      ],
      "metadata": {
        "id": "61rVUWbUd1hz"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Każdy plik jest przetwarzany funkcją process_file.\n",
        "all_files = [os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.endswith('.csv')]\n",
        "processed_dfs = [process_file(file_path, n_steps) for file_path in all_files]"
      ],
      "metadata": {
        "id": "kk2QLIJ2drfE"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Liczba kroków jest równa 10 i jest to okno dwóch tygodni (giełda nie jest otwarta w weekedy). Obliczanie zwrotu z inwestycji dla każdego symbolu akcji w oknie czasowym o długości n_steps, a wynik jest dodawany do final_df. Teraz dane mogą być przetwarzane dalej lub eksportowane. Exportujemy dane do PandasDataFrame"
      ],
      "metadata": {
        "id": "yK9OaRB1k0er"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from functools import reduce"
      ],
      "metadata": {
        "id": "sXU1nA37sQWc"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumn(\"Date\", to_date(df[\"Date\"], \"yyyy-MM-dd\"))"
      ],
      "metadata": {
        "id": "VGjB5hisx476"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pandas_df = final_df.toPandas()\n",
        "\n",
        "pandas_df = pandas_df.assign(stock_returns=pandas_df['Close'])\n",
        "\n",
        "print(pandas_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmfW7itArCfe",
        "outputId": "7278068a-1798-475a-e363-a47a32da1ea7"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Date         Open        Close                   Symbol  count  \\\n",
            "0  2013-01-02  4648.899902  4705.899902  ASX_200_historical_data   2783   \n",
            "1  2013-01-03  4705.899902  4740.700195  ASX_200_historical_data   2783   \n",
            "2  2013-01-04  4740.700195  4723.799805  ASX_200_historical_data   2783   \n",
            "3  2013-01-07  4726.799805  4717.299805  ASX_200_historical_data   2783   \n",
            "4  2013-01-08  4721.399902  4690.299805  ASX_200_historical_data   2783   \n",
            "\n",
            "     return  rolling_return  stock_returns  \n",
            "0  0.000000        0.000000    4705.899902  \n",
            "1  0.007395        0.007395    4740.700195  \n",
            "2  0.003804        0.003804    4723.799805  \n",
            "3  0.002422        0.002422    4717.299805  \n",
            "4 -0.003315       -0.003315    4690.299805  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metoda najmniejszych kwadratów z moduły scikit-learn: Wcześniej usuwania Na"
      ],
      "metadata": {
        "id": "rZYKpqXCshmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "from pyspark.sql.functions import pandas_udf\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType"
      ],
      "metadata": {
        "id": "wR2Pvj50sf5V"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pandas_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "feature_columns = list(pandas_df.columns[-6:])\n",
        "pandas_df = pandas_df.dropna(subset=feature_columns + ['stock_returns'])"
      ],
      "metadata": {
        "id": "wh5QlHUxy9Cf"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the feature columns as per your original code\n",
        "feature_columns =  ['Date', 'Close', 'Symbol', 'count', 'return', 'rolling_return', 'stock_return']\n",
        "\n",
        "# Define the UDF to compute the OLS coefficients\n",
        "@pandas_udf(StructType(\n",
        "    [StructField(\"Symbol\", StringType(), True)] +\n",
        "    [StructField(f\"coef_{feature}\", DoubleType(), True) for feature in feature_columns]\n",
        "))\n",
        "def find_ols_coef_udf(pdf: pd.DataFrame) -> pd.DataFrame:\n",
        "    # The target variable\n",
        "    y = pdf['stock_returns']\n",
        "\n",
        "    # Feature variables\n",
        "    X = pdf[feature_columns]\n",
        "\n",
        "    # Initialize the linear regression model\n",
        "    regr = LinearRegression()\n",
        "\n",
        "    # Fit the model\n",
        "    regr.fit(X, y)\n",
        "\n",
        "    # Extract coefficients\n",
        "    coef_list = [pdf['Symbol'].iloc[0]] + list(regr.coef_)\n",
        "\n",
        "    # Return as a pandas DataFrame\n",
        "    return pd.DataFrame([coef_list], columns=['Symbol'] + [f\"coef_{feature}\" for feature in feature_columns])\n",
        "\n",
        "# Apply the UDF to each group\n",
        "coef_per_stock_df = df.groupby('Symbol').apply(find_ols_coef_udf)\n",
        "\n",
        "# Show the result\n",
        "coef_per_stock_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "q6zbJaWQ6psm",
        "outputId": "c6f60255-07c6-4f5a-c491-04c2a4ff4969"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Invalid udf: the udf argument must be a pandas_udf of type GROUPED_MAP.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-128-505a67d8abaa>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Apply the UDF to each group\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mcoef_per_stock_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Symbol'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfind_ols_coef_udf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# Show the result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/pandas/group_ops.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, udf)\u001b[0m\n\u001b[1;32m     98\u001b[0m             )\n\u001b[1;32m     99\u001b[0m         ):\n\u001b[0;32m--> 100\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    101\u001b[0m                 \u001b[0;34m\"Invalid udf: the udf argument must be a pandas_udf of type \"\u001b[0m \u001b[0;34m\"GROUPED_MAP.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: Invalid udf: the udf argument must be a pandas_udf of type GROUPED_MAP."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import pandas_udf\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Define the feature columns (adjust this as per your dataset)\n",
        "\n",
        "# Define the UDF to compute the OLS coefficients\n",
        "@pandas_udf(StructType(\n",
        "    [StructField(\"Symbol\", StringType(), True)] +\n",
        "    [StructField(f\"coef_{feature}\", DoubleType(), True) for feature in feature_columns]\n",
        "), functionType='GROUPED_MAP')\n",
        "def find_ols_coef_udf(pdf: pd.DataFrame) -> pd.DataFrame:\n",
        "    # The target variable\n",
        "    y = pdf['stock_returns']\n",
        "\n",
        "    # Feature variables\n",
        "    X = pdf[feature_columns]\n",
        "\n",
        "    # Initialize the linear regression model\n",
        "    regr = LinearRegression()\n",
        "\n",
        "    # Fit the model\n",
        "    regr.fit(X, y)\n",
        "\n",
        "    # Extract coefficients\n",
        "    coef_list = [pdf['Symbol'].iloc[0]] + list(regr.coef_)\n",
        "\n",
        "    # Return as a pandas DataFrame\n",
        "    return pd.DataFrame([coef_list], columns=['Symbol'] + [f\"coef_{feature}\" for feature in feature_columns])\n",
        "\n",
        "# Use groupBy and transform (instead of apply)\n",
        "coef_per_stock_df = df.groupBy('Symbol').apply(find_ols_coef_udf)\n",
        "\n",
        "# Show the result\n",
        "coef_per_stock_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "xBrraLa9ERdw",
        "outputId": "2ad96701-f1a9-4115-d4d8-d6ebc680cc45"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "error",
          "ename": "PySparkTypeError",
          "evalue": "[INVALID_PANDAS_UDF_TYPE] `functionType` should be one the values from PandasUDFType, got GROUPED_MAP",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-129-d547135fee5d>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Define the UDF to compute the OLS coefficients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m @pandas_udf(StructType(\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0mStructField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Symbol\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStringType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0mStructField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"coef_{feature}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoubleType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature_columns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/pandas/functions.py\u001b[0m in \u001b[0;36mpandas_udf\u001b[0;34m(f, returnType, functionType)\u001b[0m\n\u001b[1;32m    382\u001b[0m     ]:  # None means it should infer the type from type hints.\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m         raise PySparkTypeError(\n\u001b[0m\u001b[1;32m    385\u001b[0m             \u001b[0merror_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"INVALID_PANDAS_UDF_TYPE\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             message_parameters={\n",
            "\u001b[0;31mPySparkTypeError\u001b[0m: [INVALID_PANDAS_UDF_TYPE] `functionType` should be one the values from PandasUDFType, got GROUPED_MAP"
          ]
        }
      ]
    }
  ]
}